{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e3e35e-5cac-43eb-ba4c-31e215548c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>{'notes': '', 'label': [1]}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>I dont. But what is complaining about it goi...</td>\n",
       "      <td>{'notes': '', 'label': ['0']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>Bahah  yeah i&amp;;m totally just gonna&amp;; get pis...</td>\n",
       "      <td>{'notes': '', 'label': ['0']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>hahahahaha &gt;:) im evil mwahahahahahahahahaha</td>\n",
       "      <td>{'notes': '', 'label': ['0']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>What&amp;;s something unique about Ohio? :)</td>\n",
       "      <td>{'notes': '', 'label': ['0']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>Who is the biggest gossiper you know?</td>\n",
       "      <td>{'notes': '', 'label': ['0']}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20001 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  \\\n",
       "0                                 Get fucking real dude.   \n",
       "1       She is as dirty as they come  and that crook ...   \n",
       "2       why did you fuck it up. I could do it all day...   \n",
       "3       Dude they dont finish enclosing the fucking s...   \n",
       "4       WTF are you talking about Men? No men thats n...   \n",
       "...                                                  ...   \n",
       "19996    I dont. But what is complaining about it goi...   \n",
       "19997   Bahah  yeah i&;m totally just gonna&; get pis...   \n",
       "19998       hahahahaha >:) im evil mwahahahahahahahahaha   \n",
       "19999            What&;s something unique about Ohio? :)   \n",
       "20000              Who is the biggest gossiper you know?   \n",
       "\n",
       "                          annotation  extras  \n",
       "0        {'notes': '', 'label': [1]}     NaN  \n",
       "1      {'notes': '', 'label': ['1']}     NaN  \n",
       "2      {'notes': '', 'label': ['1']}     NaN  \n",
       "3      {'notes': '', 'label': ['1']}     NaN  \n",
       "4      {'notes': '', 'label': ['1']}     NaN  \n",
       "...                              ...     ...  \n",
       "19996  {'notes': '', 'label': ['0']}     NaN  \n",
       "19997  {'notes': '', 'label': ['0']}     NaN  \n",
       "19998  {'notes': '', 'label': ['0']}     NaN  \n",
       "19999  {'notes': '', 'label': ['0']}     NaN  \n",
       "20000  {'notes': '', 'label': ['0']}     NaN  \n",
       "\n",
       "[20001 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_json(  \"Dataset for Detection of Cyber-Trolls.json\" , lines= True,orient='columns')\n",
    "df\n",
    "df['annotation'][0]['label'][0] =int (df['annotation'][0]['label'][0])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5927f6e9-18a6-4c3f-9285-c8a9f265d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)  # Convert all text data to string\n",
    "df['text'] = df['text'].fillna('')  # Replace NaN values with an empty string\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)  # Now apply function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1f8c947-bad7-4c9f-95ab-417321e83982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['content', 'annotation', 'extras', 'text', 'cleaned_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e209626d-2519-43c5-917d-107c8e9f2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4490bf69-32eb-4a7d-8b2a-3e858b718cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f8035cf-a5a8-4474-9109-45c0999b1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     text          cleaned_text\n",
      "0                Hello!!!                 hello\n",
      "1  Cyberbullying is bad!!  cyberbullying is bad\n",
      "2                    None                  none\n",
      "3              H@ck3r 123                  hckr\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample Data\n",
    "data = {'text': ['Hello!!!', 'Cyberbullying is bad!!', None, 'H@ck3r 123']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fix missing values & ensure string format\n",
    "df['text'] = df['text'].astype(str).fillna('')\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply Cleaning\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display Result\n",
    "print(df[['text', 'cleaned_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2942fc27-f14e-496e-b8fc-0f1294a616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a5b2d82-5bfe-4621-8721-f8c0a55cfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['cleaned_text'].astype(str).fillna('')\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e246761b-6c53-4ebc-8444-8afe16cb80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'those', 'them', 'until', 'and', 'what', 'is', 'herself', 'me', \"that'll\", 'only', \"shan't\", 'off', \"mightn't\", 'ain', \"should've\", 'on', 'if', \"shouldn't\", 'won', 'mustn', 'yourself', \"needn't\", 'because', 'after', 'her', 've', 'but', 'below', 's', 'wasn', 'itself', 'how', 'each', 'theirs', 'or', 'his', \"don't\", 'own', 'hadn', 'than', 'during', 'where', 'into', 'further', 'of', 'their', 'had', 'as', 'in', \"you'll\", 'so', \"couldn't\", 'needn', 'have', 'other', 'when', 'for', 'some', 'to', 'y', 'm', 'from', 'up', 'again', 'aren', 'very', 'did', 'couldn', 'i', 'why', 'you', 'does', 'which', 'll', 'my', 'd', 'out', \"it's\", 'there', 'nor', 'it', 'ours', 'himself', 'do', 'don', 'an', 'who', 'shan', \"hadn't\", \"you've\", 'were', 'with', 'am', 'doing', 'shouldn', 'she', 't', 'these', 'not', 'just', 'themselves', 'both', 'being', \"isn't\", 'ma', 'weren', 'under', 'should', 'he', 'didn', 'our', \"doesn't\", 'against', \"weren't\", 'are', 'down', 'yours', 'will', \"you're\", 'between', 'the', 'at', 'by', 'too', 'whom', 'no', 'hers', 'doesn', \"she's\", 'few', 'mightn', \"hasn't\", 'having', 'its', 'all', \"mustn't\", 'has', 'your', 'before', 'more', \"wasn't\", \"wouldn't\", 'that', \"you'd\", 'a', 're', 'be', 'myself', 'been', 'while', \"didn't\", 'we', 'yourselves', 'same', \"haven't\", 'haven', 'o', 'then', 'isn', 'this', \"aren't\", 'here', 'him', 'was', 'about', 'they', 'such', 'can', 'any', 'most', \"won't\", 'once', 'above', 'hasn', 'over', 'now', 'through', 'wouldn', 'ourselves'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(stop_words)  # Should print a set of stopwords like {'is', 'the', 'and', 'a', ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ec4f0eb-29be-444b-9c8c-5794ab105961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'those', 'them', 'until', 'and', 'what', 'is', 'herself', 'me', \"that'll\", 'only', \"shan't\", 'off', \"mightn't\", 'ain', \"should've\", 'on', 'if', \"shouldn't\", 'won', 'mustn', 'yourself', \"needn't\", 'because', 'after', 'her', 've', 'but', 'below', 's', 'wasn', 'itself', 'how', 'each', 'theirs', 'or', 'his', \"don't\", 'own', 'hadn', 'than', 'during', 'where', 'into', 'further', 'of', 'their', 'had', 'as', 'in', \"you'll\", 'so', \"couldn't\", 'needn', 'have', 'other', 'when', 'for', 'some', 'to', 'y', 'm', 'from', 'up', 'again', 'aren', 'very', 'did', 'couldn', 'i', 'why', 'you', 'does', 'which', 'll', 'my', 'd', 'out', \"it's\", 'there', 'nor', 'it', 'ours', 'himself', 'do', 'don', 'an', 'who', 'shan', \"hadn't\", \"you've\", 'were', 'with', 'am', 'doing', 'shouldn', 'she', 't', 'these', 'not', 'just', 'themselves', 'both', 'being', \"isn't\", 'ma', 'weren', 'under', 'should', 'he', 'didn', 'our', \"doesn't\", 'against', \"weren't\", 'are', 'down', 'yours', 'will', \"you're\", 'between', 'the', 'at', 'by', 'too', 'whom', 'no', 'hers', 'doesn', \"she's\", 'few', 'mightn', \"hasn't\", 'having', 'its', 'all', \"mustn't\", 'has', 'your', 'before', 'more', \"wasn't\", \"wouldn't\", 'that', \"you'd\", 'a', 're', 'be', 'myself', 'been', 'while', \"didn't\", 'we', 'yourselves', 'same', \"haven't\", 'haven', 'o', 'then', 'isn', 'this', \"aren't\", 'here', 'him', 'was', 'about', 'they', 'such', 'can', 'any', 'most', \"won't\", 'once', 'above', 'hasn', 'over', 'now', 'through', 'wouldn', 'ourselves'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)  # Check again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e87beb6-d59d-471d-9e3c-48fc9464b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This example sentence stopwords.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is an example sentence with stopwords.\"\n",
    "print(remove_stopwords(sample_text))  # Should return: \"example sentence stopwords.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43d4beff-0a3c-4488-9a1f-6c066d646343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  cleaned_text\n",
      "0            example sentence.\n",
      "1  Cyberbullying must stopped!\n",
      "2                         None\n",
      "3                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Ensure stopwords are properly loaded\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample Data\n",
    "df = pd.DataFrame({'cleaned_text': ['This is an example sentence.', 'Cyberbullying must be stopped!', None, '']})\n",
    "\n",
    "# Fix NaN values before applying stopwords removal\n",
    "df['cleaned_text'] = df['cleaned_text'].astype(str).fillna('')\n",
    "\n",
    "# Stopwords Removal Function\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# Apply Function\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display Results\n",
    "print(df[['cleaned_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "279e00a1-c860-4efb-a48d-d147b99cab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  cleaned_text\n",
      "0            example sentence.\n",
      "1  Cyberbullying must stopped!\n",
      "2                         None\n",
      "3                             \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a225e51-6a59-4608-b4d2-bdf72a5fc7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4e8e6ab-c3b8-4813-893e-c8400d71f891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Lemmatization:\n",
      "                  cleaned_text\n",
      "0            example sentence.\n",
      "1  Cyberbullying must stopped!\n",
      "2                         None\n",
      "3                             \n",
      "\n",
      "After Lemmatization:\n",
      "                  cleaned_text\n",
      "0            example sentence.\n",
      "1  Cyberbullying must stopped!\n",
      "2                         None\n",
      "3                             \n"
     ]
    }
   ],
   "source": [
    "# Display first few rows before applying lemmatization\n",
    "print(\"Before Lemmatization:\")\n",
    "print(df[['cleaned_text']].head())\n",
    "\n",
    "# Apply Lemmatization\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "# Display first few rows after applying lemmatization\n",
    "print(\"\\nAfter Lemmatization:\")\n",
    "print(df[['cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f9a2845-6520-4104-b237-b6533e76ac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Lemmatization: The children are running and playing\n",
      "After Lemmatization: The child are running and playing\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The children are running and playing\"\n",
    "lemmatized_text = lemmatize_text(sample_text)\n",
    "print(\"Before Lemmatization:\", sample_text)\n",
    "print(\"After Lemmatization:\", lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bbd474b-d7df-4c6e-998e-25109005c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Lemmatization: The dogs are running quickly\n",
      "After Lemmatization: The dog are running quickly\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The dogs are running quickly\"\n",
    "lemmatized_text = lemmatize_text(sample_text)\n",
    "print(\"Before Lemmatization:\", sample_text)\n",
    "print(\"After Lemmatization:\", lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c355af6-47eb-43c1-b6b0-17df31c4d09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  cleaned_text                   lemmatized\n",
      "0            example sentence.            example sentence.\n",
      "1  Cyberbullying must stopped!  Cyberbullying must stopped!\n",
      "2                         None                         None\n",
      "3                                                          \n"
     ]
    }
   ],
   "source": [
    "# Check before and after lemmatization for some rows\n",
    "df_sample = df[['cleaned_text']].head(10)\n",
    "df_sample['lemmatized'] = df_sample['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "print(df_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48fa8b4d-d082-41e9-ba6d-b984922ba13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\"u\": \"you\", \"ur\": \"your\", \"btw\": \"by the way\"}  # Add more as needed\n",
    "\n",
    "def replace_slang(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([slang_dict[word] if word in slang_dict else word for word in words])\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(replace_slang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e74b4629-2985-49d9-8d0f-16df47c34151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Slang Replacement:\n",
      "                  cleaned_text\n",
      "0            example sentence.\n",
      "1  Cyberbullying must stopped!\n",
      "2                         None\n",
      "3                             \n",
      "\n",
      "After Slang Replacement:\n",
      "                  cleaned_text\n",
      "0            example sentence.\n",
      "1  Cyberbullying must stopped!\n",
      "2                         None\n",
      "3                             \n"
     ]
    }
   ],
   "source": [
    "# Display first few rows before applying slang replacement\n",
    "print(\"Before Slang Replacement:\")\n",
    "print(df[['cleaned_text']].head())\n",
    "\n",
    "# Apply Slang Replacement\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(replace_slang)\n",
    "\n",
    "# Display first few rows after applying slang replacement\n",
    "print(\"\\nAfter Slang Replacement:\")\n",
    "print(df[['cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84d1131a-8c8f-4d6f-8e23-cd8e15925701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Slang Replacement: u r going to love this btw\n",
      "After Slang Replacement: you r going to love this by the way\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"u r going to love this btw\"\n",
    "replaced_text = replace_slang(sample_text)\n",
    "print(\"Before Slang Replacement:\", sample_text)\n",
    "print(\"After Slang Replacement:\", replaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373adb5-3b24-4b59-aeec-feb36e74060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Before Slang Replacement: u r going to love this btw\n",
    "After Slang Replacement: you your going to love this by the way\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
